{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12 Latent Dirichlet Allocation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNl3b7td4ditw1iYDOQvKXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/jisang/12_Latent_Dirichlet_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCTZQ-HHDTR",
        "colab_type": "text"
      },
      "source": [
        "# **Topic Modeling - Latent Dirichlet Allocation 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29eDj3kOHMbN",
        "colab_type": "text"
      },
      "source": [
        "## **1. 잠재 디리클레 할당(LDA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLex5ycVb-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice, bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6pisRs2HHtB",
        "colab_type": "text"
      },
      "source": [
        "### **1-1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ty4cVnZ4B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4b2ded98-9c43-4347-ca42-b8d1fee17064"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvxjjZnBWJWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "11948f39-e279-49b6-f290-635b28d901ef"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "pos_docs = []\n",
        "for line in docs_ls:\n",
        "    doc = line.split(\" \")\n",
        "    tmp_docs = []\n",
        "    for word in doc:\n",
        "        # 소문자화, Lemmatize\n",
        "        tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "    # 영어 품사 부착(PoS Tagging)\n",
        "    pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "pos_docs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('cute', 'NN'), ('kitty', 'NN')],\n",
              " [('eat', 'NN'), ('rice', 'NN'), ('or', 'CC'), ('cake', 'VB')],\n",
              " [('kitty', 'NNS'), ('and', 'CC'), ('hamster', 'NN')],\n",
              " [('eat', 'NN'), ('bread', 'NN')],\n",
              " [('rice,', 'NN'), ('bread', 'NN'), ('and', 'CC'), ('cake', 'NN')],\n",
              " [('cute', 'NN'),\n",
              "  ('hamster', 'NN'),\n",
              "  ('eat', 'NN'),\n",
              "  ('bread', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('cake', 'NN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeym2j3N-dXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "c123829b-dbb1-4590-8edd-4e71cfe0d65b"
      },
      "source": [
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_token = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    doc_token_tmp = []\n",
        "    for pos_token in pos_doc:\n",
        "        # 불용 품사 지정\n",
        "        if pos_token[1] not in stopPos:\n",
        "            # 불용어 지정\n",
        "            if pos_token[0] not in stopWord:\n",
        "                doc_token_tmp.append(pos_token[0])\n",
        "                tokens.append(pos_token[0])\n",
        "    # 문서 사용 단어\n",
        "    docs_token.append(doc_token_tmp)\n",
        "# 전체 문서 단어\n",
        "tokens = list(set(tokens))\n",
        "\n",
        "docs_token, tokens"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['cute', 'kitty'],\n",
              "  ['eat', 'rice', 'cake'],\n",
              "  ['kitty', 'hamster'],\n",
              "  ['eat', 'bread'],\n",
              "  ['rice,', 'bread', 'cake'],\n",
              "  ['cute', 'hamster', 'eat', 'bread', 'cake']],\n",
              " ['bread', 'cake', 'hamster', 'kitty', 'rice,', 'cute', 'rice', 'eat'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiyMUQipHWEj",
        "colab_type": "text"
      },
      "source": [
        "### **1-2. LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUAtmpE__Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65bf7cf5-1137-4f47-eb97-d138b88d5504"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# 토픽 랜덤 설정\n",
        "topic = 2 # 임의의 랜덤값\n",
        "topic_set = []\n",
        "\n",
        "# 토픽 랜덤 부여\n",
        "for i in range(len(docs_token)):\n",
        "    topic_count = [randint(1, topic) for a in range(len(docs_token[i]))]\n",
        "    topic_set.append(topic_count)\n",
        "\n",
        "topic_set"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1], [2, 1, 1], [1, 1], [1, 1], [2, 2, 1], [1, 2, 2, 2, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSruBwdDCg_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "9fdbf8f6-7160-4572-87b3-c9f0d88c28e5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 문서 내 토픽 분포\n",
        "alpha = 0.1 # 알파 값 부여\n",
        "\n",
        "topic_doc = []\n",
        "for i in range(len(topic_set)):\n",
        "    tmp = []\n",
        "    for j in range(1, topic+1):\n",
        "        if j in topic_set[i]:\n",
        "            tmp.append(topic_set[i].count(j) + alpha)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "    topic_doc.append(tmp)\n",
        "\n",
        "topic_doc"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-79ced9237220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtopic_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'topic_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOqC6FQLEy6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8af385b3-db02-4b91-c81c-de58543ddc29"
      },
      "source": [
        "# 토픽 내 단어 분포\n",
        "beta = 0.001 # 베타값 부여\n",
        "\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic+1):\n",
        "            if topic_set[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_word"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.001, 2.001, 1.001, 2.001, 0.001, 1.001, 1.001, 1.001],\n",
              " [2.001, 1.001, 1.001, 0.001, 1.001, 1.001, 0.001, 2.001]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTh1WFAXnh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "9ee99161-e855-4cfc-c623-250d45052331"
      },
      "source": [
        "# 합계\n",
        "prob_td = []\n",
        "prob_tw = []\n",
        "\n",
        "# 문서내 토픽 확률\n",
        "for i in range(len(topic_doc)):\n",
        "    td_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_doc[i])):\n",
        "        prob_tmp.append(topic_doc[i][j]/td_total)\n",
        "    prob_td.append(prob_tmp)\n",
        "\n",
        "# 토픽 내 단어 합계\n",
        "for i in range(len(topic_word)):\n",
        "    tw_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_word[i])):\n",
        "        prob_tmp.append(topic_word[i][j]/tw_total)\n",
        "    prob_tw.append(prob_tmp)\n",
        "\n",
        "prob_td, prob_tw"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0.5, 0.5],\n",
              "  [0.65625, 0.34375],\n",
              "  [1.0, 0.0],\n",
              "  [1.0, 0.0],\n",
              "  [0.34375, 0.65625],\n",
              "  [0.2115384615384616, 0.7884615384615385]],\n",
              " [[0.4549999999999999,\n",
              "   0.9095454545454544,\n",
              "   0.4549999999999999,\n",
              "   0.9095454545454544,\n",
              "   0.00045454545454545455,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999],\n",
              "  [0.6253124999999999,\n",
              "   0.31281249999999994,\n",
              "   0.31281249999999994,\n",
              "   0.0003125,\n",
              "   0.31281249999999994,\n",
              "   0.31281249999999994,\n",
              "   0.0003125,\n",
              "   0.6253124999999999]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAw5sXewf_t3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "a0319ea3-1edb-4d20-ca9a-e88d921e38b3"
      },
      "source": [
        "# 토픽 부여 행렬\n",
        "topic_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    topic_prob = [[0 for a in range((topic))] for a in range(len(docs_token[i]))]\n",
        "    topic_result.append(topic_prob)\n",
        "topic_result\n",
        "\n",
        "# LDA 계산\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        for k in range(topic):\n",
        "                topic_result[i][j][k] = prob_td[i][k] * prob_tw[k][tokens.index(docs_token[i][j])]\n",
        "\n",
        "topic_result"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.22749999999999995, 0.15640624999999997],\n",
              "  [0.4547727272727272, 0.00015625]],\n",
              " [[0.29859374999999994, 0.21495117187499999],\n",
              "  [0.29859374999999994, 0.000107421875],\n",
              "  [0.5968892045454545, 0.10752929687499999]],\n",
              " [[0.9095454545454544, 0.0], [0.4549999999999999, 0.0]],\n",
              " [[0.4549999999999999, 0.0], [0.4549999999999999, 0.0]],\n",
              " [[0.00015625, 0.20528320312499995],\n",
              "  [0.15640624999999997, 0.410361328125],\n",
              "  [0.31265624999999997, 0.20528320312499995]],\n",
              " [[0.09625, 0.24664062499999997],\n",
              "  [0.09625, 0.24664062499999997],\n",
              "  [0.09625, 0.4930348557692308],\n",
              "  [0.09625, 0.4930348557692308],\n",
              "  [0.19240384615384618, 0.24664062499999997]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o5xCwiIgfPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe2aeee8-326f-43bf-da8b-cb0f035cd067"
      },
      "source": [
        "# 최종 토픽 할당\n",
        "LDA_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    LDA_result.append([[0 for a in range((topic))] for a in range(len(docs_token[i]))])\n",
        "\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        LDA_result[i][j] = topic_result[i][j].index(np.max(topic_result[i][j])) + 1\n",
        "\n",
        "LDA_result"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1], [1, 1, 1], [1, 1], [1, 1], [2, 2, 1], [2, 2, 2, 2, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xlfLLMLovHe",
        "colab_type": "text"
      },
      "source": [
        "## **2. 잠재 디리클레 할당(LDA) 클래스화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAOtjtlYxKU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "592f1020-8ea5-4ccc-e5bd-8e8f140c01a7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQaU0Otp3b6",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5QO4h9nMmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LDA():\n",
        "    def __init__(self, topic):\n",
        "        self.topic = topic\n",
        "        self.pos_docs = []\n",
        "        self.docs_token = []\n",
        "        self.tokens = []\n",
        "        self.topic_set = []\n",
        "        self.topic_doc = []\n",
        "        self.topic_word = []\n",
        "        self.prob_td = []\n",
        "        self.prob_tw = []\n",
        "        self.topic_result = []\n",
        "        self.LDA_result = []\n",
        "        self.result_word = []\n",
        "        self.result_topic = []\n",
        "\n",
        "        # 불용어, 불용품사\n",
        "        self.stopPos = ['CC', 'IN', 'TO']\n",
        "        self.stopWord = [',', '.', 'be']\n",
        "\n",
        "    # 문장 전처리\n",
        "    def preprocessing_data(self, docs_ls):\n",
        "        wl = WordNetLemmatizer()\n",
        "        for line in docs_ls:\n",
        "            doc = line.split(\" \")\n",
        "            tmp_docs = []\n",
        "            for word in doc:\n",
        "                # 소문자화, Lemmatize\n",
        "                tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "            # 영어 품사 부착(PoS Tagging)\n",
        "            self.pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "        return self.pos_docs\n",
        "\n",
        "    # 토큰화\n",
        "    def tokenize(self):\n",
        "        for pos_doc in self.pos_docs:\n",
        "            doc_token_tmp = []\n",
        "            for pos_token in pos_doc:\n",
        "                # 불용 품사 지정\n",
        "                if pos_token[1] not in self.stopPos:\n",
        "                    # 불용어 지정\n",
        "                    if pos_token[0] not in self.stopWord:\n",
        "                        doc_token_tmp.append(pos_token[0])\n",
        "                        self.tokens.append(pos_token[0])\n",
        "                # 문서 사용 단어\n",
        "            self.docs_token.append(doc_token_tmp)\n",
        "            # 전체 문서 단어\n",
        "        self.tokens = list(set(self.tokens))\n",
        "\n",
        "        return self.docs_token, self.tokens\n",
        "\n",
        "    # 토픽 설정\n",
        "    def give_topic(self):\n",
        "        tmp_topic_set = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_count = [randint(1, self.topic) for a in range(len(self.docs_token[i]))]\n",
        "            tmp_topic_set.append(topic_count)\n",
        "        self.topic_set = tmp_topic_set\n",
        "\n",
        "        return self.topic_set\n",
        "\n",
        "    # 문서 내 토픽 분포\n",
        "    def chk_topicdoc(self, alpha):\n",
        "        tmp_topic_doc = []\n",
        "        for i in range(len(self.topic_set)):\n",
        "            tmp = []\n",
        "            for j in range(1, self.topic+1):\n",
        "                if j in self.topic_set[i]:\n",
        "                    tmp.append(self.topic_set[i].count(j) + alpha)\n",
        "                else:\n",
        "                    tmp.append('0')\n",
        "            tmp_topic_doc.append(tmp)\n",
        "        self.topic_doc = tmp_topic_doc\n",
        "\n",
        "        return self.topic_doc\n",
        "\n",
        "    # 토픽 내 단어 분포\n",
        "    def chk_topicword(self, beta):\n",
        "        self.topic_word = [[0 for a in range((len(self.tokens)))] for b in range(self.topic)]\n",
        "        for i in range(len(self.docs_token)):\n",
        "            for j in range(len(self.docs_token[i])):\n",
        "                for k in range(1, self.topic+1):\n",
        "                    if self.topic_set[i][j] == k:\n",
        "                            self.topic_word[k-1][self.tokens.index(self.docs_token[i][j])] += 1\n",
        "        for i in range(len(self.topic_word)):\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                self.topic_word[i][j] += beta\n",
        "        \n",
        "        return self.topic_word\n",
        "\n",
        "    # 문서 내 토픽, 토픽 내 단어 확률\n",
        "    def cal_probabilty(self):\n",
        "        tmp_prob_td = []\n",
        "        for i in range(len(self.topic_doc)):\n",
        "            td_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_doc[i])):\n",
        "                prob_tmp.append(self.topic_doc[i][j]/td_total)\n",
        "            tmp_prob_td.append(prob_tmp)\n",
        "        self.prob_td = tmp_prob_td\n",
        "        # 토픽 내 단어 합계\n",
        "        tmp_prob_tw = []\n",
        "        for i in range(len(self.topic_word)):\n",
        "            tw_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                prob_tmp.append(self.topic_word[i][j]/tw_total)\n",
        "            tmp_prob_tw.append(prob_tmp)\n",
        "        self.prob_tw = tmp_prob_tw\n",
        "\n",
        "        return self.prob_td, self.prob_tw\n",
        "\n",
        "    # 토픽 부여 행렬\n",
        "    def cal_LDA(self):\n",
        "        # LDA 행렬 생성\n",
        "        tmp_topic_result = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_prob = [[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))]\n",
        "            tmp_topic_result.append(topic_prob)\n",
        "        self.topic_result = tmp_topic_result\n",
        "        # LDA 계산\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                for k in range(self.topic):\n",
        "                    self.topic_result[i][j][k] = self.prob_td[i][k] * self.prob_tw[k][self.tokens.index(self.docs_token[i][j])]\n",
        "\n",
        "        return self.topic_result\n",
        "\n",
        "    # 결과 토픽 할당\n",
        "    def result_LDA(self):\n",
        "        tmp_LDA_result = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            tmp_LDA_result.append([[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))])\n",
        "        self.LDA_result = tmp_LDA_result\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.LDA_result[i][j] = self.topic_result[i][j].index(np.max(self.topic_result[i][j])) + 1\n",
        "        \n",
        "        return self.LDA_result\n",
        "        \n",
        "    # 결과 출력\n",
        "    def print_result(self):\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.result_word.append(self.docs_token[i][j])\n",
        "                self.result_topic.append(self.LDA_result[i][j])\n",
        "\n",
        "    # 자동 실행\n",
        "    def run(self, docs_ls, alpha, beta):\n",
        "        self.preprocessing_data(docs_ls)\n",
        "        self.tokenize()\n",
        "        self.give_topic()\n",
        "        count_time = 0\n",
        "        while True:\n",
        "            self.chk_topicdoc(alpha)\n",
        "            self.chk_topicword(beta)\n",
        "            self.cal_probabilty()\n",
        "            self.cal_LDA()\n",
        "            self.result_LDA()\n",
        "            count_time += 1\n",
        "            if self.LDA_result == self.topic_set:\n",
        "                self.print_result()\n",
        "                print(\"반복 횟수 : {}회\".format(count_time))\n",
        "                break\n",
        "            else:\n",
        "                self.topic_set = self.LDA_result.copy()\n",
        "        \n",
        "        return pd.DataFrame([self.result_topic], columns=self.result_word)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyglW_Agp9km",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZXxmHnZpsfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LDA(4)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFuRU5Jp2QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Major highways running across the capital Seoul were partly closed Thursday as heavy rains pushed up the water level of the city's Han River.\",\n",
        "            \"Several sections of the Dongbu Urban Expressway, Seoul Inner Loop, Olympic-daero and Gangbyeon Northern Highway have been closed to traffic due to the inner city river's swelled water level, according to the police and the city of Seoul.\",\n",
        "            \"As heavy rains continued to batter the country's metropolitan and central regions, authorities opened the floodgates of Soyang River Dam and Paldang Dam a day earlier, releasing the waters to empty into the sea through the Han River.\",\n",
        "            \"Eleven people remained missing as of Thursday morning due to the cloudburst, while more than 1,600 people were displaced from their homes in areas hit hardest, such as North and South Chungcheong provinces, Gyeonggi Province and Gangwon Province.\",\n",
        "            \"Three train lines, including Taebaek and Chungbuk, remained totally or partially out of service, while 39 roads across the affected areas were off limits due to mudslide damage from the torrential rains, according to authorities.\"\n",
        "            \"As of Thursday morning, more than 5,000 houses and facilities were reported flooded or damaged in the latest bout of heavy rains. Nearly 8,065 hectares of farm land have been inundated or ravaged.\"\n",
        "            \"The weather agency urged extra caution and safety measures, as the central and southern regions are expected to receive rainfall of up to 50 millimeters per hour with strong winds on Thursday and Friday.\"\n",
        "          ]"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eUlPR86uiA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "e82dc8d3-754b-462b-c51e-6167e2168e5d"
      },
      "source": [
        "lda.run(docs_ls, 0.1, 0.001)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "반복 횟수 : 4회\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>major</th>\n",
              "      <th>highways</th>\n",
              "      <th>run</th>\n",
              "      <th>the</th>\n",
              "      <th>capital</th>\n",
              "      <th>seoul</th>\n",
              "      <th>partly</th>\n",
              "      <th>close</th>\n",
              "      <th>thursday</th>\n",
              "      <th>heavy</th>\n",
              "      <th>rain</th>\n",
              "      <th>push</th>\n",
              "      <th>up</th>\n",
              "      <th>the</th>\n",
              "      <th>water</th>\n",
              "      <th>level</th>\n",
              "      <th>the</th>\n",
              "      <th>city's</th>\n",
              "      <th>han</th>\n",
              "      <th>river.</th>\n",
              "      <th>several</th>\n",
              "      <th>section</th>\n",
              "      <th>the</th>\n",
              "      <th>dongbu</th>\n",
              "      <th>urban</th>\n",
              "      <th>expressway,</th>\n",
              "      <th>seoul</th>\n",
              "      <th>inner</th>\n",
              "      <th>loop,</th>\n",
              "      <th>olympic-daero</th>\n",
              "      <th>gangbyeon</th>\n",
              "      <th>northern</th>\n",
              "      <th>highway</th>\n",
              "      <th>have</th>\n",
              "      <th>close</th>\n",
              "      <th>traffic</th>\n",
              "      <th>due</th>\n",
              "      <th>the</th>\n",
              "      <th>inner</th>\n",
              "      <th>city</th>\n",
              "      <th>...</th>\n",
              "      <th>5,000</th>\n",
              "      <th>house</th>\n",
              "      <th>facilities</th>\n",
              "      <th>report</th>\n",
              "      <th>flood</th>\n",
              "      <th>damage</th>\n",
              "      <th>the</th>\n",
              "      <th>latest</th>\n",
              "      <th>bout</th>\n",
              "      <th>heavy</th>\n",
              "      <th>rains.</th>\n",
              "      <th>nearly</th>\n",
              "      <th>8,065</th>\n",
              "      <th>hectares</th>\n",
              "      <th>farm</th>\n",
              "      <th>land</th>\n",
              "      <th>have</th>\n",
              "      <th>inundate</th>\n",
              "      <th>ravaged.the</th>\n",
              "      <th>weather</th>\n",
              "      <th>agency</th>\n",
              "      <th>urge</th>\n",
              "      <th>extra</th>\n",
              "      <th>caution</th>\n",
              "      <th>safety</th>\n",
              "      <th>measures,</th>\n",
              "      <th>the</th>\n",
              "      <th>central</th>\n",
              "      <th>southern</th>\n",
              "      <th>regions</th>\n",
              "      <th>expect</th>\n",
              "      <th>receive</th>\n",
              "      <th>rainfall</th>\n",
              "      <th>50</th>\n",
              "      <th>millimeters</th>\n",
              "      <th>hour</th>\n",
              "      <th>strong</th>\n",
              "      <th>wind</th>\n",
              "      <th>thursday</th>\n",
              "      <th>friday.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 175 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   major  highways  run  the  capital  ...  hour  strong  wind  thursday  friday.\n",
              "0      4         3    1    3        3  ...     4       1     3         3        3\n",
              "\n",
              "[1 rows x 175 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCLuEkIwwuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}