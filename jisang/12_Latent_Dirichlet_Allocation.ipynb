{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12 Latent Dirichlet Allocation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPN94GOD2coA11t2r81fJmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/jisang/12_Latent_Dirichlet_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCTZQ-HHDTR",
        "colab_type": "text"
      },
      "source": [
        "# **Topic Modeling - Latent Dirichlet Allocation 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29eDj3kOHMbN",
        "colab_type": "text"
      },
      "source": [
        "## **1. 잠재 디리클레 할당(LDA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLex5ycVb-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice, bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6pisRs2HHtB",
        "colab_type": "text"
      },
      "source": [
        "### **1-1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ty4cVnZ4B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "b7fc3c87-25da-4aed-f244-c030d3887c05"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvxjjZnBWJWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "f0c7a1e9-5005-4910-ef51-d0bd555c26c2"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "pos_docs = []\n",
        "for line in docs_ls:\n",
        "    doc = line.split(\" \")\n",
        "    tmp_docs = []\n",
        "    for word in doc:\n",
        "        # 소문자화, Lemmatize\n",
        "        tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "    # 영어 품사 부착(PoS Tagging)\n",
        "    pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "pos_docs"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('cute', 'NN'), ('kitty', 'NN')],\n",
              " [('eat', 'NN'), ('rice', 'NN'), ('or', 'CC'), ('cake', 'VB')],\n",
              " [('kitty', 'NNS'), ('and', 'CC'), ('hamster', 'NN')],\n",
              " [('eat', 'NN'), ('bread', 'NN')],\n",
              " [('rice,', 'NN'), ('bread', 'NN'), ('and', 'CC'), ('cake', 'NN')],\n",
              " [('cute', 'NN'),\n",
              "  ('hamster', 'NN'),\n",
              "  ('eat', 'NN'),\n",
              "  ('bread', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('cake', 'NN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeym2j3N-dXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3591bf7f-ed30-40ba-95cd-5cf1f7d324c5"
      },
      "source": [
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_token = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    doc_token_tmp = []\n",
        "    for pos_token in pos_doc:\n",
        "        # 불용 품사 지정\n",
        "        if pos_token[1] not in stopPos:\n",
        "            # 불용어 지정\n",
        "            if pos_token[0] not in stopWord:\n",
        "                doc_token_tmp.append(pos_token[0])\n",
        "                tokens.append(pos_token[0])\n",
        "    # 문서 사용 단어\n",
        "    docs_token.append(doc_token_tmp)\n",
        "# 전체 문서 단어\n",
        "tokens = list(set(tokens))\n",
        "\n",
        "docs_token, tokens"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['cute', 'kitty'],\n",
              "  ['eat', 'rice', 'cake'],\n",
              "  ['kitty', 'hamster'],\n",
              "  ['eat', 'bread'],\n",
              "  ['rice,', 'bread', 'cake'],\n",
              "  ['cute', 'hamster', 'eat', 'bread', 'cake']],\n",
              " ['cute', 'rice,', 'hamster', 'bread', 'cake', 'rice', 'eat', 'kitty'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiyMUQipHWEj",
        "colab_type": "text"
      },
      "source": [
        "### **1-2. LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUAtmpE__Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8721f810-e704-4319-e1d3-3788358c48ee"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# 토픽 랜덤 설정\n",
        "topic = 2 # 임의의 랜덤값\n",
        "topic_set = []\n",
        "\n",
        "# 토픽 랜덤 부여\n",
        "for i in range(len(docs_token)):\n",
        "    topic_count = [randint(1, topic) for a in range(len(docs_token[i]))]\n",
        "    topic_set.append(topic_count)\n",
        "\n",
        "topic_set"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 2], [2, 1, 2], [1, 2], [2, 2], [1, 1, 2], [1, 2, 2, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSruBwdDCg_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37a14d1f-c978-4cb5-a6c2-c3ec03bd0b7c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 문서 내 토픽 분포\n",
        "alpha = 0.1 # 알파 값 부여\n",
        "\n",
        "topic_doc = []\n",
        "for i in range(len(topic_set)):\n",
        "    tmp = []\n",
        "    for j in range(1, topic+1):\n",
        "        if j in topic_set[i]:\n",
        "            tmp.append(topic_set[i].count(j) + seta)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "    topic_doc.append(tmp)\n",
        "\n",
        "topic_doc"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 2.1], [1.1, 2.1], [1.1, 1.1], [0, 2.1], [2.1, 1.1], [3.1, 2.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOqC6FQLEy6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0d583007-0c8a-4280-ce5e-ea854499b3b0"
      },
      "source": [
        "# 토픽 내 단어 분포\n",
        "beta = 0.001 # 베타값 부여\n",
        "\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic+1):\n",
        "            if topic_set[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_word"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.001, 1.001, 0.001, 2.001, 1.001, 1.001, 0.001, 1.001],\n",
              " [1.001, 0.001, 2.001, 1.001, 2.001, 0.001, 3.001, 1.001]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTh1WFAXnh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "ae06c696-2103-41c1-fc61-3231f0993eac"
      },
      "source": [
        "# 합계\n",
        "prob_td = []\n",
        "prob_tw = []\n",
        "\n",
        "# 문서내 토픽 확률\n",
        "for i in range(len(topic_doc)):\n",
        "    td_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_doc[i])):\n",
        "        prob_tmp.append(topic_doc[i][j]/td_total)\n",
        "    prob_td.append(prob_tmp)\n",
        "\n",
        "# 토픽 내 단어 합계\n",
        "for i in range(len(topic_word)):\n",
        "    tw_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_word[i])):\n",
        "        prob_tmp.append(topic_word[i][j]/tw_total)\n",
        "    prob_tw.append(prob_tmp)\n",
        "\n",
        "prob_td, prob_tw"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0.0, 1.0],\n",
              "  [0.34375, 0.65625],\n",
              "  [0.5, 0.5],\n",
              "  [0.0, 1.0],\n",
              "  [0.65625, 0.34375],\n",
              "  [0.5961538461538461, 0.40384615384615385]],\n",
              " [[0.47666666666666657,\n",
              "   0.47666666666666657,\n",
              "   0.0004761904761904762,\n",
              "   0.9528571428571427,\n",
              "   0.47666666666666657,\n",
              "   0.47666666666666657,\n",
              "   0.0004761904761904762,\n",
              "   0.47666666666666657],\n",
              "  [0.31281249999999994,\n",
              "   0.0003125,\n",
              "   0.6253124999999999,\n",
              "   0.31281249999999994,\n",
              "   0.6253124999999999,\n",
              "   0.0003125,\n",
              "   0.9378124999999999,\n",
              "   0.31281249999999994]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAw5sXewf_t3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "ff3c780a-a179-4230-9928-a4be52386b43"
      },
      "source": [
        "# 토픽 부여 행렬\n",
        "for i in range(len(docs_token)):\n",
        "    topic_prob = [[0 for a in range((topic))] for a in range(len(docs_token[i]))]\n",
        "    topic_result.append(topic_prob)\n",
        "topic_result\n",
        "\n",
        "# LDA 계산\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        for k in range(topic):\n",
        "                topic_result[i][j][k] = prob_td[i][k] * prob_tw[k][tokens.index(docs_token[i][j])]\n",
        "\n",
        "topic_result"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.0, 0.31281249999999994], [0.0, 0.31281249999999994]],\n",
              " [[0.00016369047619047618, 0.6154394531249999],\n",
              "  [0.16385416666666663, 0.000205078125],\n",
              "  [0.16385416666666663, 0.410361328125]],\n",
              " [[0.23833333333333329, 0.15640624999999997],\n",
              "  [0.0002380952380952381, 0.31265624999999997]],\n",
              " [[0.0, 0.9378124999999999], [0.0, 0.31281249999999994]],\n",
              " [[0.31281249999999994, 0.000107421875],\n",
              "  [0.6253124999999999, 0.10752929687499999],\n",
              "  [0.31281249999999994, 0.21495117187499999]],\n",
              " [[0.2841666666666666, 0.12632812499999999],\n",
              "  [0.0002838827838827839, 0.25253004807692303],\n",
              "  [0.0002838827838827839, 0.37873197115384616],\n",
              "  [0.5680494505494504, 0.12632812499999999],\n",
              "  [0.2841666666666666, 0.25253004807692303]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o5xCwiIgfPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c78099e-2330-4556-c49e-4cec61f103c2"
      },
      "source": [
        "# 최종 토픽 할당\n",
        "LDA_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    LDA_result.append([[0 for a in range((topic))] for a in range(len(docs_token[i]))])\n",
        "\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        LDA_result[i][j] = topic_result[i][j].index(np.max(topic_result[i][j])) + 1\n",
        "\n",
        "LDA_result"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 2], [2, 1, 2], [1, 2], [2, 2], [1, 1, 1], [1, 2, 2, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5QO4h9nMmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}