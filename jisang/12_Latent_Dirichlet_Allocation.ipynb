{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12 Latent Dirichlet Allocation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVrOSIf9gzS/FOYtEdhaj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/jisang/12_Latent_Dirichlet_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCTZQ-HHDTR",
        "colab_type": "text"
      },
      "source": [
        "# **Topic Modeling - Latent Dirichlet Allocation 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29eDj3kOHMbN",
        "colab_type": "text"
      },
      "source": [
        "## **1. 잠재 디리클레 할당(LDA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLex5ycVb-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice, bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\"]"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6pisRs2HHtB",
        "colab_type": "text"
      },
      "source": [
        "### **1-1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ty4cVnZ4B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "d3e42021-d7c4-468a-eac9-9046a00ac39d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvxjjZnBWJWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "6519917d-58db-49c8-f2c1-2f05470a2e06"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "pos_docs = []\n",
        "for line in docs_ls:\n",
        "    doc = line.split(\" \")\n",
        "    tmp_docs = []\n",
        "    for word in doc:\n",
        "        # 소문자화, Lemmatize\n",
        "        tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "    # 영어 품사 부착(PoS Tagging)\n",
        "    pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "pos_docs"
      ],
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('cute', 'NN'), ('kitty', 'NN')],\n",
              " [('eat', 'NN'), ('rice', 'NN'), ('or', 'CC'), ('cake', 'VB')],\n",
              " [('kitty', 'NNS'), ('and', 'CC'), ('hamster', 'NN')],\n",
              " [('eat', 'NN'), ('bread', 'NN')],\n",
              " [('rice,', 'NN'), ('bread', 'NN'), ('and', 'CC'), ('cake', 'NN')],\n",
              " [('cute', 'NN'),\n",
              "  ('hamster', 'NN'),\n",
              "  ('eat', 'NN'),\n",
              "  ('bread', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('cake', 'NN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeym2j3N-dXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "0d26f2ea-abdd-42e3-b75a-84c795912d13"
      },
      "source": [
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_token = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    doc_token_tmp = []\n",
        "    for pos_token in pos_doc:\n",
        "        # 불용 품사 지정\n",
        "        if pos_token[1] not in stopPos:\n",
        "            # 불용어 지정\n",
        "            if pos_token[0] not in stopWord:\n",
        "                doc_token_tmp.append(pos_token[0])\n",
        "                tokens.append(pos_token[0])\n",
        "    # 문서 사용 단어\n",
        "    docs_token.append(doc_token_tmp)\n",
        "# 전체 문서 단어\n",
        "tokens = list(set(tokens))\n",
        "\n",
        "docs_token, tokens"
      ],
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['cute', 'kitty'],\n",
              "  ['eat', 'rice', 'cake'],\n",
              "  ['kitty', 'hamster'],\n",
              "  ['eat', 'bread'],\n",
              "  ['rice,', 'bread', 'cake'],\n",
              "  ['cute', 'hamster', 'eat', 'bread', 'cake']],\n",
              " ['eat', 'rice', 'kitty', 'cute', 'hamster', 'cake', 'bread', 'rice,'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiyMUQipHWEj",
        "colab_type": "text"
      },
      "source": [
        "### **1-2. LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUAtmpE__Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81bae9e6-6f97-4cc1-be95-6fdd52d4b44e"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# 토픽 랜덤 설정\n",
        "topic = 2 # 임의의 랜덤값\n",
        "topic_set = []\n",
        "\n",
        "# 토픽 랜덤 부여\n",
        "for i in range(len(docs_token)):\n",
        "    topic_count = [randint(1, topic) for a in range(len(docs_token[i]))]\n",
        "    topic_set.append(topic_count)\n",
        "\n",
        "topic_set"
      ],
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1], [2, 2, 1], [1, 1], [2, 1], [1, 1, 1], [2, 1, 2, 2, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSruBwdDCg_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3f94006-faec-43d5-ca86-309849e90caa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 문서 내 토픽 분포\n",
        "alpha = 0.1 # 알파 값 부여\n",
        "\n",
        "topic_doc = []\n",
        "for i in range(len(topic_set)):\n",
        "    tmp = []\n",
        "    for j in range(1, topic+1):\n",
        "        if j in topic_set[i]:\n",
        "            tmp.append(topic_set[i].count(j) + alpha)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "    topic_doc.append(tmp)\n",
        "\n",
        "topic_doc"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.1, 0], [1.1, 2.1], [2.1, 0], [1.1, 1.1], [3.1, 0], [2.1, 3.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOqC6FQLEy6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e6028fc-75ac-4eb7-d03a-17f3233aadcb"
      },
      "source": [
        "# 토픽 내 단어 분포\n",
        "beta = 0.001 # 베타값 부여\n",
        "\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic+1):\n",
        "            if topic_set[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_word"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001, 0.001, 2.001, 1.001, 2.001, 3.001, 2.001, 1.001],\n",
              " [3.001, 1.001, 0.001, 1.001, 0.001, 0.001, 1.001, 0.001]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTh1WFAXnh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "85d315f1-b35d-417e-9a6f-70e41a68f3ff"
      },
      "source": [
        "# 합계\n",
        "prob_td = []\n",
        "prob_tw = []\n",
        "\n",
        "# 문서내 토픽 확률\n",
        "for i in range(len(topic_doc)):\n",
        "    td_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_doc[i])):\n",
        "        prob_tmp.append(topic_doc[i][j]/td_total)\n",
        "    prob_td.append(prob_tmp)\n",
        "\n",
        "# 토픽 내 단어 합계\n",
        "for i in range(len(topic_word)):\n",
        "    tw_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_word[i])):\n",
        "        prob_tmp.append(topic_word[i][j]/tw_total)\n",
        "    prob_tw.append(prob_tmp)\n",
        "\n",
        "prob_td, prob_tw"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1.0, 0.0],\n",
              "  [0.34375, 0.65625],\n",
              "  [1.0, 0.0],\n",
              "  [0.5, 0.5],\n",
              "  [1.0, 0.0],\n",
              "  [0.40384615384615385, 0.5961538461538461]],\n",
              " [[0.0004761904761904762,\n",
              "   0.0004761904761904762,\n",
              "   0.9528571428571427,\n",
              "   0.47666666666666657,\n",
              "   0.9528571428571427,\n",
              "   1.429047619047619,\n",
              "   0.9528571428571427,\n",
              "   0.47666666666666657],\n",
              "  [0.9378124999999999,\n",
              "   0.31281249999999994,\n",
              "   0.0003125,\n",
              "   0.31281249999999994,\n",
              "   0.0003125,\n",
              "   0.0003125,\n",
              "   0.31281249999999994,\n",
              "   0.0003125]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAw5sXewf_t3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "41a73048-53cc-46bb-e4e2-52486346214f"
      },
      "source": [
        "# 토픽 부여 행렬\n",
        "topic_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    topic_prob = [[0 for a in range((topic))] for a in range(len(docs_token[i]))]\n",
        "    topic_result.append(topic_prob)\n",
        "topic_result\n",
        "\n",
        "# LDA 계산\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        for k in range(topic):\n",
        "                topic_result[i][j][k] = prob_td[i][k] * prob_tw[k][tokens.index(docs_token[i][j])]\n",
        "\n",
        "topic_result"
      ],
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.47666666666666657, 0.0], [0.9528571428571427, 0.0]],\n",
              " [[0.00016369047619047618, 0.6154394531249999],\n",
              "  [0.00016369047619047618, 0.20528320312499995],\n",
              "  [0.491235119047619, 0.000205078125]],\n",
              " [[0.9528571428571427, 0.0], [0.9528571428571427, 0.0]],\n",
              " [[0.0002380952380952381, 0.46890624999999997],\n",
              "  [0.47642857142857137, 0.15640624999999997]],\n",
              " [[0.47666666666666657, 0.0],\n",
              "  [0.9528571428571427, 0.0],\n",
              "  [1.429047619047619, 0.0]],\n",
              " [[0.19249999999999998, 0.18648437499999995],\n",
              "  [0.38480769230769224, 0.00018629807692307693],\n",
              "  [0.0001923076923076923, 0.5590805288461538],\n",
              "  [0.38480769230769224, 0.18648437499999995],\n",
              "  [0.5771153846153846, 0.00018629807692307693]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o5xCwiIgfPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0ea4f90-c32d-4efd-a4a0-ecc3f0db38d5"
      },
      "source": [
        "# 최종 토픽 할당\n",
        "LDA_result = []\n",
        "LDA_prob = []\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    LDA_result.append([[0 for a in range((topic))] for a in range(len(docs_token[i]))])\n",
        "\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        LDA_result[i][j] = topic_result[i][j].index(np.max(topic_result[i][j])) + 1\n",
        "\n",
        "LDA_result, LDA_prob"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1, 1], [2, 2, 1], [1, 1], [2, 1], [1, 1, 1], [1, 1, 2, 1, 1]], [])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJpxAK8jtYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "1fd0eadb-26fe-43e1-bf6e-f49244c025c5"
      },
      "source": [
        "result_word = []\n",
        "result_topic = []\n",
        "result_prob = []\n",
        "\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        result_word.append(docs_token[i][j])\n",
        "        result_topic.append(LDA_result[i][j])\n",
        "\n",
        "prob_tw, tokens\n",
        "topic_idx = [str(i+1) for i in range(topic)]\n",
        "df = pd.DataFrame(prob_tw, columns=tokens, index=topic_idx)\n",
        "df"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eat</th>\n",
              "      <th>rice</th>\n",
              "      <th>kitty</th>\n",
              "      <th>cute</th>\n",
              "      <th>hamster</th>\n",
              "      <th>cake</th>\n",
              "      <th>bread</th>\n",
              "      <th>rice,</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.952857</td>\n",
              "      <td>0.476667</td>\n",
              "      <td>0.952857</td>\n",
              "      <td>1.429048</td>\n",
              "      <td>0.952857</td>\n",
              "      <td>0.476667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.937812</td>\n",
              "      <td>0.312812</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.312812</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.312812</td>\n",
              "      <td>0.000313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        eat      rice     kitty  ...      cake     bread     rice,\n",
              "1  0.000476  0.000476  0.952857  ...  1.429048  0.952857  0.476667\n",
              "2  0.937812  0.312812  0.000313  ...  0.000313  0.312812  0.000313\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIEmx14T4iMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "a15f3a7f-5bb3-4ce2-8f10-16d85c169233"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 길이에 대한 히스토그램 확인\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt(df, label=df.columns)\n",
        "plt.show()"
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-438-e0a0081524a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqjnKxzkdcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "208aea00-2d06-4a1c-fe60-38985bbc11fd"
      },
      "source": [
        "predict_word = \"cute kitty eat\"\n",
        "\n",
        "for word in predict_word.split(\" \"):\n",
        "    tmp = list(df[word])\n",
        "    print(\"{}의 주제는 토픽 {}입니다.\".format(word, topic_idx[tmp.index(max(tmp))]))"
      ],
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cute의 주제는 토픽 1입니다.\n",
            "kitty의 주제는 토픽 1입니다.\n",
            "eat의 주제는 토픽 2입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xlfLLMLovHe",
        "colab_type": "text"
      },
      "source": [
        "## **2. 잠재 디리클레 할당(LDA) 클래스화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAOtjtlYxKU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b3c1d391-7b06-4fe9-a815-01eeacbde6ef"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQaU0Otp3b6",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5QO4h9nMmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LDA():\n",
        "    def __init__(self, topic):\n",
        "        self.topic = topic\n",
        "        self.docs_token = []\n",
        "        self.tokens = []\n",
        "        self.topic_set = []\n",
        "        self.topic_doc = []\n",
        "        self.topic_word = []\n",
        "        self.prob_td = []\n",
        "        self.prob_tw = []\n",
        "        self.topic_result = []\n",
        "        self.LDA_result = []\n",
        "        self.result_word = []\n",
        "        self.result_topic = []\n",
        "\n",
        "        # 불용어, 불용품사\n",
        "        self.stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']\n",
        "        self.stopWord = ['.', ',','be','able']\n",
        "\n",
        "    # 문장 전처리 및 토큰화\n",
        "    def tokenize(self, docs_ls):\n",
        "        pos_docs = []\n",
        "        wl = WordNetLemmatizer()\n",
        "        for line in docs_ls:\n",
        "            doc = line.split(\" \")\n",
        "            tmp_docs = []\n",
        "            for word in doc:\n",
        "                # 소문자화, Lemmatize\n",
        "                tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "            # 영어 품사 부착(PoS Tagging)\n",
        "            pos_docs.append(pos_tag(tmp_docs))\n",
        "        # 토큰화\n",
        "        for pos_doc in pos_docs:\n",
        "            doc_token_tmp = []\n",
        "            for pos_token in pos_doc:\n",
        "                # 불용 품사 지정\n",
        "                if pos_token[1] not in self.stopPos:\n",
        "                    # 불용어 지정\n",
        "                    if pos_token[0] not in self.stopWord:\n",
        "                        doc_token_tmp.append(pos_token[0])\n",
        "                        self.tokens.append(pos_token[0])\n",
        "                # 문서 사용 단어\n",
        "            self.docs_token.append(doc_token_tmp)\n",
        "            # 전체 문서 단어\n",
        "        self.tokens = list(set(self.tokens))\n",
        "\n",
        "        return self.docs_token, self.tokens\n",
        "\n",
        "    # 토픽 설정\n",
        "    def give_topic(self):\n",
        "        tmp_topic_set = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_count = [randint(1, self.topic) for a in range(len(self.docs_token[i]))]\n",
        "            tmp_topic_set.append(topic_count)\n",
        "        self.topic_set = tmp_topic_set\n",
        "\n",
        "        return self.topic_set\n",
        "\n",
        "    # 문서 내 토픽 분포\n",
        "    def chk_topicdoc(self, alpha):\n",
        "        tmp_topic_doc = []\n",
        "        for i in range(len(self.topic_set)):\n",
        "            tmp = []\n",
        "            for j in range(1, self.topic+1):\n",
        "                if j in self.topic_set[i]:\n",
        "                    tmp.append(self.topic_set[i].count(j) + alpha)\n",
        "                else:\n",
        "                    tmp.append('0')\n",
        "            tmp_topic_doc.append(tmp)\n",
        "        self.topic_doc = tmp_topic_doc\n",
        "\n",
        "        return self.topic_doc\n",
        "\n",
        "    # 토픽 내 단어 분포\n",
        "    def chk_topicword(self, beta):\n",
        "        self.topic_word = [[0 for a in range((len(self.tokens)))] for b in range(self.topic)]\n",
        "        for i in range(len(self.docs_token)):\n",
        "            for j in range(len(self.docs_token[i])):\n",
        "                for k in range(1, self.topic+1):\n",
        "                    if self.topic_set[i][j] == k:\n",
        "                            self.topic_word[k-1][self.tokens.index(self.docs_token[i][j])] += 1\n",
        "        for i in range(len(self.topic_word)):\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                self.topic_word[i][j] += beta\n",
        "        \n",
        "        return self.topic_word\n",
        "\n",
        "    # 문서 내 토픽, 토픽 내 단어 확률\n",
        "    def cal_probabilty(self):\n",
        "        tmp_prob_td = []\n",
        "        for i in range(len(self.topic_doc)):\n",
        "            td_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_doc[i])):\n",
        "                prob_tmp.append(self.topic_doc[i][j]/td_total)\n",
        "            tmp_prob_td.append(prob_tmp)\n",
        "        self.prob_td = tmp_prob_td\n",
        "        # 토픽 내 단어 합계\n",
        "        tmp_prob_tw = []\n",
        "        for i in range(len(self.topic_word)):\n",
        "            tw_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                prob_tmp.append(self.topic_word[i][j]/tw_total)\n",
        "            tmp_prob_tw.append(prob_tmp)\n",
        "        self.prob_tw = tmp_prob_tw\n",
        "\n",
        "        return self.prob_td, self.prob_tw\n",
        "\n",
        "    # 토픽 부여 행렬\n",
        "    def cal_LDA(self):\n",
        "        # LDA 행렬 생성\n",
        "        tmp_topic_result = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_prob = [[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))]\n",
        "            tmp_topic_result.append(topic_prob)\n",
        "        self.topic_result = tmp_topic_result\n",
        "        # LDA 계산\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                for k in range(self.topic):\n",
        "                    self.topic_result[i][j][k] = self.prob_td[i][k] * self.prob_tw[k][self.tokens.index(self.docs_token[i][j])]\n",
        "\n",
        "        return self.topic_result\n",
        "\n",
        "    # 결과 토픽 할당\n",
        "    def result_LDA(self):\n",
        "        tmp_LDA_result = []\n",
        "        for i in range(len(self.docs_token)):\n",
        "            tmp_LDA_result.append([[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))])\n",
        "        self.LDA_result = tmp_LDA_result\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.LDA_result[i][j] = self.topic_result[i][j].index(np.max(self.topic_result[i][j])) + 1\n",
        "        \n",
        "        return self.LDA_result\n",
        "        \n",
        "    # 결과 출력\n",
        "    def print_result(self):\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.result_word.append(self.docs_token[i][j])\n",
        "                self.result_topic.append(self.LDA_result[i][j])\n",
        "        \n",
        "        return self.result_word, self.result_topic\n",
        "\n",
        "    # 자동 실행\n",
        "    def run(self, docs_ls, alpha, beta):\n",
        "        self.tokenize(docs_ls)\n",
        "        self.give_topic()\n",
        "        count_time = 0\n",
        "        # Iteration 과정 추가\n",
        "        while True:\n",
        "            self.chk_topicdoc(alpha)\n",
        "            self.chk_topicword(beta)\n",
        "            self.cal_probabilty()\n",
        "            self.cal_LDA()\n",
        "            self.result_LDA()\n",
        "            count_time += 1\n",
        "            if self.LDA_result == self.topic_set:\n",
        "                self.print_result()\n",
        "                print(\"반복 횟수 : {}회\".format(count_time))\n",
        "                break\n",
        "            else:\n",
        "                self.topic_set = self.LDA_result.copy()\n",
        "    \n",
        "        return pd.DataFrame([self.result_topic], columns=self.result_word)\n",
        "\n",
        "    # 예측\n",
        "    def predict_topic(self, pre_doc):\n",
        "        # 토픽 내 단어 확률\n",
        "        topic_idx = [str(i+1) for i in range(self.topic)]\n",
        "        topic_count = []\n",
        "        df = pd.DataFrame(self.prob_tw, columns=self.tokens, index=topic_idx)\n",
        "        for word in pre_doc.split(\" \"):\n",
        "            tmp = list(df[word])\n",
        "            topic_count.append(topic_idx[tmp.index(max(tmp))])\n",
        "            print(\"{}의 주제는 토픽 {}입니다.\".format(word, topic_idx[tmp.index(max(tmp))]))\n",
        "        # print(\"{}의 주제는 토픽 {}입니다\".format(pre_doc, topic_count))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyglW_Agp9km",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZXxmHnZpsfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LDA(3)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFuRU5Jp2QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Major highways running across the capital Seoul were partly closed Thursday as heavy rains pushed up the water level of the city's Han River.\",\n",
        "            \"Several sections of the Dongbu Urban Expressway, Seoul Inner Loop, Olympic-daero and Gangbyeon Northern Highway have been closed to traffic due to the inner city river's swelled water level, according to the police and the city of Seoul.\",\n",
        "            \"As heavy rains continued to batter the country's metropolitan and central regions, authorities opened the floodgates of Soyang River Dam and Paldang Dam a day earlier, releasing the waters to empty into the sea through the Han River.\",\n",
        "            \"Eleven people remained missing as of Thursday morning due to the cloudburst, while more than 1,600 people were displaced from their homes in areas hit hardest, such as North and South Chungcheong provinces, Gyeonggi Province and Gangwon Province.\",\n",
        "            \"Three train lines, including Taebaek and Chungbuk, remained totally or partially out of service, while 39 roads across the affected areas were off limits due to mudslide damage from the torrential rains, according to authorities.\",\n",
        "            \"As of Thursday morning, more than 5,000 houses and facilities were reported flooded or damaged in the latest bout of heavy rains. Nearly 8,065 hectares of farm land have been inundated or ravaged.\",\n",
        "          ]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eUlPR86uiA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "548df6d1-35aa-47b0-e8f9-64a9a247e64d"
      },
      "source": [
        "lda.run(docs_ls, 0.1, 0.001)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "반복 횟수 : 2회\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>major</th>\n",
              "      <th>highways</th>\n",
              "      <th>run</th>\n",
              "      <th>capital</th>\n",
              "      <th>seoul</th>\n",
              "      <th>partly</th>\n",
              "      <th>close</th>\n",
              "      <th>thursday</th>\n",
              "      <th>heavy</th>\n",
              "      <th>rain</th>\n",
              "      <th>push</th>\n",
              "      <th>up</th>\n",
              "      <th>water</th>\n",
              "      <th>level</th>\n",
              "      <th>city's</th>\n",
              "      <th>han</th>\n",
              "      <th>river.</th>\n",
              "      <th>several</th>\n",
              "      <th>section</th>\n",
              "      <th>dongbu</th>\n",
              "      <th>urban</th>\n",
              "      <th>expressway,</th>\n",
              "      <th>seoul</th>\n",
              "      <th>inner</th>\n",
              "      <th>loop,</th>\n",
              "      <th>olympic-daero</th>\n",
              "      <th>gangbyeon</th>\n",
              "      <th>northern</th>\n",
              "      <th>highway</th>\n",
              "      <th>close</th>\n",
              "      <th>traffic</th>\n",
              "      <th>due</th>\n",
              "      <th>inner</th>\n",
              "      <th>city</th>\n",
              "      <th>river's</th>\n",
              "      <th>water</th>\n",
              "      <th>level,</th>\n",
              "      <th>accord</th>\n",
              "      <th>police</th>\n",
              "      <th>city</th>\n",
              "      <th>...</th>\n",
              "      <th>train</th>\n",
              "      <th>lines,</th>\n",
              "      <th>taebaek</th>\n",
              "      <th>chungbuk,</th>\n",
              "      <th>totally</th>\n",
              "      <th>partially</th>\n",
              "      <th>service,</th>\n",
              "      <th>39</th>\n",
              "      <th>roads</th>\n",
              "      <th>affect</th>\n",
              "      <th>areas</th>\n",
              "      <th>off</th>\n",
              "      <th>limit</th>\n",
              "      <th>due</th>\n",
              "      <th>mudslide</th>\n",
              "      <th>damage</th>\n",
              "      <th>torrential</th>\n",
              "      <th>rains,</th>\n",
              "      <th>accord</th>\n",
              "      <th>authorities.</th>\n",
              "      <th>thursday</th>\n",
              "      <th>morning,</th>\n",
              "      <th>more</th>\n",
              "      <th>5,000</th>\n",
              "      <th>house</th>\n",
              "      <th>facilities</th>\n",
              "      <th>report</th>\n",
              "      <th>flood</th>\n",
              "      <th>damage</th>\n",
              "      <th>latest</th>\n",
              "      <th>bout</th>\n",
              "      <th>heavy</th>\n",
              "      <th>rains.</th>\n",
              "      <th>nearly</th>\n",
              "      <th>8,065</th>\n",
              "      <th>hectares</th>\n",
              "      <th>farm</th>\n",
              "      <th>land</th>\n",
              "      <th>inundate</th>\n",
              "      <th>ravaged.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   major  highways  run  capital  ...  farm  land  inundate  ravaged.\n",
              "0      1         1    1        1  ...     3     3         1         1\n",
              "\n",
              "[1 rows x 127 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxl1qOdR7CGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = \"highway seoul thursday\""
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCLuEkIwwuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2f20f981-c656-406a-8e44-9bd38aa4c3a6"
      },
      "source": [
        "lda.predict_topic(predict)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "highway의 주제는 토픽 3입니다.\n",
            "seoul의 주제는 토픽 1입니다.\n",
            "thursday의 주제는 토픽 1입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ypYPBu7I7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}