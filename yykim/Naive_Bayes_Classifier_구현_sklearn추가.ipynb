{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes Classifier 구현.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlZR1c53cJ+CQtlrPph+2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/yykim/Naive_Bayes_Classifier_%EA%B5%AC%ED%98%84_sklearn%EC%B6%94%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpqKzs0Gg0TZ",
        "colab_type": "text"
      },
      "source": [
        "#Naive Bayes Classifier 구현\n",
        "##### Spam/Normal 분류 예시\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8xL5QwjE5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tW1LWaftQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = 'me free lottery'\n",
        "m2 = 'free get free you'\n",
        "m3 = 'you free scholarhip'\n",
        "m4 = 'free to contact me'\n",
        "m5 = 'you won award'\n",
        "m6 = 'you ticket lottery'\n",
        "labels = ['spam', 'spam', 'normal', 'normal', 'normal', 'spam']\n",
        "mails = [m1, m2, m3, m4, m5, m6]\n",
        "\n",
        "#토크나이징\n",
        "for i, mail in enumerate(mails):\n",
        "    mails[i] = mail.split(\" \")\n",
        "mails"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEMmzf5Uga9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "edc32a13-4333-4523-bdc6-64ea9db18f84"
      },
      "source": [
        "#토크나이징\n",
        "for i, mail in enumerate(mails):\n",
        "    mails[i] = mail.split(\" \")\n",
        "mails"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW1gItvmTBEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#토큰 개수 세기 \n",
        "total_token = []\n",
        "for mail in mails:\n",
        "    for token in mail:\n",
        "        total_token.append(token)\n",
        "unique_token = np.unique(total_token)\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2DV1hpxSczh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e6fad221-9c5b-440d-ab49-7755e1ec706f"
      },
      "source": [
        "#각각의 토큰 구하기\n",
        "spam_token = []\n",
        "normal_token = []\n",
        "for i, mail in enumerate(mails):\n",
        "    for token in mail:\n",
        "        if labels[i] == 'spam':\n",
        "            spam_token.append(token)\n",
        "        else:\n",
        "            normal_token.append(token)\n",
        "print(spam_token)\n",
        "print(normal_token)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['me', 'free', 'lottery', 'free', 'get', 'free', 'you', 'you', 'ticket', 'lottery']\n",
            "['you', 'free', 'scholarhip', 'free', 'to', 'contact', 'me', 'you', 'won', 'award']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNSLKRr8hD_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1cf1bce-c3dc-4a62-c0d9-5c7c713acd8c"
      },
      "source": [
        "#사전확률 P(spam), P(normal) 구하기\n",
        "p_spam = len(spam_token)/len(total_token)\n",
        "p_normal = 1-p_spam\n",
        "print(p_spam, p_normal)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dggb25pkQKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#각각의 메일에서 토큰이 나온 횟수세기\n",
        "spam_n_unique_token=[]\n",
        "for token in unique_token:\n",
        "    tmp = spam_token.count(token)\n",
        "    spam_n_unique_token.append(tmp)\n",
        "\n",
        "normal_n_unique_token=[]\n",
        "for token in unique_token:\n",
        "    tmp = normal_token.count(token)\n",
        "    normal_n_unique_token.append(tmp)\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ5D8cs5p3Rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e4faee8f-67d8-4b6e-a6d1-e5f12115ae2c"
      },
      "source": [
        "#laplace smoothing위해 0.5와 1을 더해줌 \n",
        "p_token_in_spam = (np.array(spam_n_unique_token)+0.5)/(sum(np.array(spam_n_unique_token))+1)\n",
        "p_token_in_normal = (np.array(normal_n_unique_token)+0.5)/(sum(np.array(normal_n_unique_token))+1)\n",
        "p_token_in_spam\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04545455, 0.04545455, 0.31818182, 0.13636364, 0.22727273,\n",
              "       0.13636364, 0.04545455, 0.13636364, 0.04545455, 0.04545455,\n",
              "       0.22727273])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Ee2gdDp5D-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "4472dc34-c996-4286-ec3b-b06df5a7b659"
      },
      "source": [
        "spam_dict_unique_token = dict(zip(list(unique_token),np.log(p_token_in_spam)))\n",
        "normal_dict_unique_token = dict(zip(list(unique_token),np.log(p_token_in_normal)))\n",
        "spam_dict_unique_token"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'award': -3.0910424533583156,\n",
              " 'contact': -3.0910424533583156,\n",
              " 'free': -1.1451323043030026,\n",
              " 'get': -1.9924301646902063,\n",
              " 'lottery': -1.4816045409242156,\n",
              " 'me': -1.9924301646902063,\n",
              " 'scholarhip': -3.0910424533583156,\n",
              " 'ticket': -1.9924301646902063,\n",
              " 'to': -3.0910424533583156,\n",
              " 'won': -3.0910424533583156,\n",
              " 'you': -1.4816045409242156}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_xTcBISrud_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fe79532-42fe-425e-cb1c-80921804aaf1"
      },
      "source": [
        "#P(spam)*P(free|spam)*P(lottery|spam)\n",
        "word_is_spam = np.log(p_spam) + spam_dict_unique_token['free'] + spam_dict_unique_token['lottery']\n",
        "word_is_spam = np.exp(word_is_spam)\n",
        "word_is_spam"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03615702479338842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7dox4xmslG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30269941-9104-42e4-8ab6-52803039a3cb"
      },
      "source": [
        "#P(normal)*P(free|normal)*P(lottery|normal)\n",
        "word_is_normal = np.log(p_normal) + normal_dict_unique_token['free'] + normal_dict_unique_token['lottery']\n",
        "word_is_normal = np.exp(word_is_normal)\n",
        "word_is_normal"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00516528925619835"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNUPiSBEtnhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "51e99248-47f1-4f72-f226-e53b4b9c2dcf"
      },
      "source": [
        "#P(spam|words)\n",
        "word_is_spam/(word_is_spam + word_is_normal)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8749999999999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsX-E_4ZtFOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fe94f1e-3b9d-455e-8b4e-0b8bf37b0323"
      },
      "source": [
        "#P(normal|words)\n",
        "word_is_normal/(word_is_spam + word_is_normal)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12500000000000008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7By_mNiPt-1q",
        "colab_type": "text"
      },
      "source": [
        "### Class 만들기\n",
        "- 이중분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrssFHeVt9pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, docs = [], labels = [], words=[], k = 1, package= 'manual'):\n",
        "    # docs는 토큰화한 문서들을 리스트화 한 이중 리스트\n",
        "        self.docs = docs\n",
        "        self.labels = labels\n",
        "        self.k = k #laplace smoothing을 위한 상수 k\n",
        "        self.words = words\n",
        "        self.package = package\n",
        "        self.p_category1 = 0 \n",
        "        self.p_category2 = 0\n",
        "        self.p_c1_post = 0\n",
        "        self.p_c2_post = 0\n",
        "        self.category_name=[]\n",
        "\n",
        "\n",
        "    def _cal_prior(self):\n",
        "        #토큰 개수 세기 \n",
        "        total_token = []\n",
        "        for doc in self.docs:\n",
        "            for token in doc:\n",
        "                total_token.append(token)\n",
        "        unique_token = np.unique(total_token)        \n",
        "\n",
        "        #토큰 분류\n",
        "        self.category_name = np.unique(labels)\n",
        "        c1_token = []\n",
        "        c2_token = []\n",
        "        for i, doc in enumerate(self.docs):\n",
        "            for token in doc:\n",
        "                if labels[i] == self.category_name[0]:\n",
        "                    c1_token.append(token)\n",
        "                else:\n",
        "                    c2_token.append(token)\n",
        "\n",
        "        #각각의 카테고리의 사전확률 계산 \n",
        "        self.p_category1 = len(c1_token)/len(total_token)\n",
        "        self.p_category2 = 1-self.p_category1\n",
        "        return {self.category_name[0]:self.p_category1, self.category_name[1]:self.p_category2}\n",
        "\n",
        "    def _cal_posterior(self):\n",
        "        self._cal_prior()\n",
        "        #문서 분류\n",
        "        docs_1 = []\n",
        "        docs_2 = []\n",
        "        for i, doc in enumerate(self.docs):\n",
        "            if labels[i] == self.category_name[0]:\n",
        "                docs_1.append(doc)\n",
        "            else:\n",
        "                docs_2.append(doc)        \n",
        "\n",
        "        #단어 토큰 개수 세기 \n",
        "        total_token = []\n",
        "        for doc in self.docs:\n",
        "            for token in doc:\n",
        "                total_token.append(token)\n",
        "        unique_token = np.unique(total_token)\n",
        "\n",
        "        category1_total_token = []\n",
        "        for mail in docs_1:\n",
        "            for token in mail:\n",
        "                category1_total_token.append(token)\n",
        "    \n",
        "        category2_total_token = []\n",
        "        for mail in docs_2:\n",
        "            for token in mail:\n",
        "                category2_total_token.append(token)\n",
        "\n",
        "        #각각의 메일에서 토큰이 나온 횟수세기\n",
        "        category1_n_unique_token=[]\n",
        "        for token in unique_token:\n",
        "            tmp = category1_total_token.count(token)\n",
        "            category1_n_unique_token.append(tmp)\n",
        "\n",
        "        category2_n_unique_token=[]\n",
        "        for token in unique_token:\n",
        "            tmp = category2_total_token.count(token)\n",
        "            category2_n_unique_token.append(tmp)\n",
        "\n",
        "        p_token_in_c1 = (np.array(category1_n_unique_token)+1*self.k)/(sum(np.array(category1_n_unique_token))+2*self.k)\n",
        "        p_token_in_c2 = (np.array(category2_n_unique_token)+1*self.k)/(sum(np.array(category2_n_unique_token))+2*self.k)\n",
        "\n",
        "        c1_dict_unique_token = dict(zip(list(unique_token),np.log(p_token_in_c1)))\n",
        "        c2_dict_unique_token = dict(zip(list(unique_token),np.log(p_token_in_c2)))\n",
        "\n",
        "        word_is_c1 = np.log(self.p_category1)\n",
        "        for word in self.words:\n",
        "            word_is_c1 += c1_dict_unique_token[word] \n",
        "        word_is_c1 = np.exp(word_is_c1)\n",
        "\n",
        "        word_is_c2 = np.log(self.p_category2) \n",
        "        for word in self.words:\n",
        "            word_is_c2 += c2_dict_unique_token[word] \n",
        "        word_is_c2 = np.exp(word_is_c2)\n",
        "\n",
        "        self.p_c1_post = word_is_c1/(word_is_c1 + word_is_c2)\n",
        "        self.p_c2_post = word_is_c2/(word_is_c1 + word_is_c2)\n",
        "        return {self.category_name[0]:self.p_c1_post, self.category_name[1]:self.p_c2_post}\n",
        "\n",
        "    def _use_plain_sklearn(self):\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        from sklearn.feature_extraction.text import CountVectorizer\n",
        "        from sklearn.feature_extraction.text import TfidfTransformer\n",
        "        from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "        text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                            ('tfidf', TfidfTransformer()),\n",
        "                            ('clf', MultinomialNB()), ])\n",
        "        og_docs = [\" \".join(i) for i in self.docs]\n",
        "        self.docs = og_docs\n",
        "        text_clf = text_clf.fit(self.docs, self.labels)\n",
        "\n",
        "        self.words = [\" \".join(self.words)]\n",
        "        return text_clf.predict(self.words)\n",
        "\n",
        "    def _use_gs_sklearn(self, parameters_dict):\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        from sklearn.feature_extraction.text import CountVectorizer\n",
        "        from sklearn.feature_extraction.text import TfidfTransformer\n",
        "        from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "        text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                            ('tfidf', TfidfTransformer()),\n",
        "                            ('clf', MultinomialNB()), ])\n",
        "        og_docs = [\" \".join(i) for i in self.docs]\n",
        "        self.docs = og_docs\n",
        "        text_clf = text_clf.fit(self.docs, self.labels)\n",
        "\n",
        "        \n",
        "        # parameters_dict = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
        "        from sklearn.model_selection import GridSearchCV\n",
        "        gs_clf = GridSearchCV(text_clf, parameters_dict, n_jobs=-1, verbose=2)\n",
        "        gs_clf = gs_clf.fit(self.docs, self.labels)\n",
        "\n",
        "        self.words = [\" \".join(self.words)]\n",
        "        return gs_clf.best_estimator_.get_params(), gs_clf.predict(self.words)\n",
        "\n",
        "    def classify(self):\n",
        "        if self.package == 'manual':\n",
        "            self._cal_prior()\n",
        "            self._cal_posterior()\n",
        "            if self.p_c1_post >= self.p_c2_post:\n",
        "                classified_as = self.category_name[0]\n",
        "            else:\n",
        "                classified_as = self.category_name[1]\n",
        "            return classified_as\n",
        "\n",
        "        elif self.package == 'sklearn':\n",
        "            return self._use_plain_sklearn()\n",
        "\n",
        "        elif self.package == 'sklearn_gs':\n",
        "            from ast import literal_eval\n",
        "            parameters_dict = input(\"조정할 파라미터 딕셔너리를 입력해주세요: \")\n",
        "            parameters_dict = literal_eval(parameters_dict) \n",
        "            return self._use_gs_sklearn(parameters_dict)\n",
        "\n",
        "        else:\n",
        "            print(\"지원하지 않는 방식입니다.\")\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnzu65Ivv6v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc = NaiveBayesClassifier(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXjkfliJF1Pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79064dfc-28e7-4b52-8e08-b2c52d94cba9"
      },
      "source": [
        "nbc._cal_posterior()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'normal': 0.12500000000000008, 'spam': 0.8749999999999999}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLG5Kq9xFtq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a51a3e89-35a2-4ebe-da5a-aab2fbc0e141"
      },
      "source": [
        "nbc.classify()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nFtw0CPKIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc1 = NaiveBayesClassifier(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRsLDiVE9qw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80c95b1c-1e71-41cb-e7b9-135cfbc247f2"
      },
      "source": [
        "nbc1.classify()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xx1gbqrGxtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc2 = NaiveBayesClassifier(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn_gs')"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfG-m9YTS69N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "6b433952-f20b-49bb-a515-cfca1b76ff9c"
      },
      "source": [
        "nbc2.classify() ##데이터가 너무 적어서 오류 생김"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "조정할 파라미터 딕셔너리를 입력해주세요: {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-692a967e8cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnbc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-163-ce40117666fb>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"조정할 파라미터 딕셔너리를 입력해주세요: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_gs_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-163-ce40117666fb>\u001b[0m in \u001b[0;36m_use_gs_sklearn\u001b[0;34m(self, parameters_dict)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    661\u001b[0m             raise ValueError(\"n_splits=%d cannot be greater than the\"\n\u001b[1;32m    662\u001b[0m                              \u001b[0;34m\" number of members in each class.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                              % (self.n_splits))\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             warnings.warn((\"The least populated class in y has only %d\"\n",
            "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogYGR6oO9uLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc2 = NaiveBayesClassifier(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn_gs')"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx-saSmiCn-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "2a8898ff-b44c-4a5e-cd1e-0fe0a385ce18"
      },
      "source": [
        "nbc2.classify()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "조정할 파라미터 딕셔너리를 입력해주세요: {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-692a967e8cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnbc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-128-2cc4c029f737>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"조정할 파라미터 딕셔너리를 입력해주세요: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_gs_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-2cc4c029f737>\u001b[0m in \u001b[0;36m_use_gs_sklearn\u001b[0;34m(self, parameters_dict)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# parameters_dict = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_splits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNSbvh-EG_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "404e3d70-cc60-43e9-dad2-d5ea265beb5f"
      },
      "source": [
        "m1 = 'me free lottery'\n",
        "m2 = 'free get free you'\n",
        "m3 = 'you free scholarhip'\n",
        "m4 = 'free to contact me'\n",
        "m5 = 'you won award'\n",
        "m6 = 'you ticket lottery'\n",
        "labels = ['spam', 'spam', 'normal', 'normal', 'normal', 'spam', 'spam', 'spam', 'normal', 'normal', 'normal', 'spam']\n",
        "mails = [m1, m2, m3, m4, m5, m6, m1, m2, m3, m4, m5, m6]\n",
        "\n",
        "#토크나이징\n",
        "for i, mail in enumerate(mails):\n",
        "    mails[i] = mail.split(\" \")\n",
        "mails"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery'],\n",
              " ['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7vMqO1dN2ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터 늘린 후 다시시도\n",
        "nbc2 = NaiveBayesClassifier(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn_gs')"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LNn7p_N4DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "outputId": "6eff67c7-6a08-4c11-9942-1364da90b092"
      },
      "source": [
        "nbc2.classify()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "조정할 파라미터 딕셔너리를 입력해주세요: {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
              "  'clf__alpha': 1.0,\n",
              "  'clf__class_prior': None,\n",
              "  'clf__fit_prior': True,\n",
              "  'memory': None,\n",
              "  'steps': [('vect',\n",
              "    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                    dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                    lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                    ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                    strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                    tokenizer=None, vocabulary=None)),\n",
              "   ('tfidf',\n",
              "    TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              "   ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "  'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
              "  'tfidf__norm': 'l2',\n",
              "  'tfidf__smooth_idf': True,\n",
              "  'tfidf__sublinear_tf': False,\n",
              "  'tfidf__use_idf': True,\n",
              "  'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                  tokenizer=None, vocabulary=None),\n",
              "  'vect__analyzer': 'word',\n",
              "  'vect__binary': False,\n",
              "  'vect__decode_error': 'strict',\n",
              "  'vect__dtype': numpy.int64,\n",
              "  'vect__encoding': 'utf-8',\n",
              "  'vect__input': 'content',\n",
              "  'vect__lowercase': True,\n",
              "  'vect__max_df': 1.0,\n",
              "  'vect__max_features': None,\n",
              "  'vect__min_df': 1,\n",
              "  'vect__ngram_range': (1, 1),\n",
              "  'vect__preprocessor': None,\n",
              "  'vect__stop_words': None,\n",
              "  'vect__strip_accents': None,\n",
              "  'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "  'vect__tokenizer': None,\n",
              "  'vect__vocabulary': None,\n",
              "  'verbose': False},\n",
              " array(['spam'], dtype='<U6'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWYlTlnUN5WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = \"I love you\"\n",
        "l2 = \"love happy weekend\"\n",
        "l3 = \"bore work job\"\n",
        "l4 = \"I hate you\"\n",
        "l5 = \"bore weekend\"\n",
        "l6 = \"happy together\"\n",
        "labels_s = ['긍정', '긍정', '부정', '부정', '부정', '긍정']\n",
        "bunch = [l1, l2, l3, l4, l5, l6]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_bGZk30SPCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ab183ab4-b335-4018-e024-c92f5bdff262"
      },
      "source": [
        "#spam/normal이 아닌 경우\n",
        "ls = [i.split(\" \") for i in bunch]\n",
        "ls\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', 'love', 'you'],\n",
              " ['love', 'happy', 'weekend'],\n",
              " ['bore', 'work', 'job'],\n",
              " ['I', 'hate', 'you'],\n",
              " ['bore', 'weekend'],\n",
              " ['happy', 'together']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1fMgl-TOd6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc1 = NaiveBayesClassifier(docs=ls, labels=labels_s, words=['happy', 'weekend'], k=0.5)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKlIA7zOOpLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f9289ea-1d5e-4547-a0da-47f1d5e7f137"
      },
      "source": [
        "nbc1._cal_prior()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'긍정': 0.5, '부정': 0.5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz-LTSN4OrTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad3d87f5-c9a1-4b2d-bba2-1cbcfc0ca893"
      },
      "source": [
        "nbc1._cal_posterior()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'긍정': 0.8333333333333334, '부정': 0.1666666666666666}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZwkHjzVQ-Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0da24329-e5da-433c-b7d1-a21ce40bb42c"
      },
      "source": [
        "nbc1.classify()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'긍정'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwXV517YSiXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}