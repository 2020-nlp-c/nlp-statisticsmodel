{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes Classifier 구현.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN44V2gu52ti0ItTd2WyTUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/yykim/Naive_Bayes_Classifier_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpqKzs0Gg0TZ",
        "colab_type": "text"
      },
      "source": [
        "#Naive Bayes Classifier 구현\n",
        "##### Spam/Normal 분류 예시\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8xL5QwjE5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tW1LWaftQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b6df688c-1e0a-4cdf-f14f-61f7d6279801"
      },
      "source": [
        "m1 = 'me free lottery'\n",
        "m2 = 'free get free you'\n",
        "m3 = 'you free scholarhip'\n",
        "m4 = 'free to contact me'\n",
        "m5 = 'you won award'\n",
        "m6 = 'you ticket lottery'\n",
        "labels = ['spam', 'spam', 'normal', 'normal', 'normal', 'spam']\n",
        "mails = [m1, m2, m3, m4, m5, m6]\n",
        "\n",
        "#토크나이징\n",
        "for i, mail in enumerate(mails):\n",
        "    mails[i] = mail.split(\" \")\n",
        "mails"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwXV517YSiXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOgODHQtpRr8",
        "colab_type": "text"
      },
      "source": [
        "###Class 만들기 (다중분류, sklearn 포함)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20IjTbY9pVIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class NaiveBayesClassifier_multi:\n",
        "    def __init__(self, docs = [], labels = [], words=[], k = 1, package= 'manual'):\n",
        "    # docs는 토큰화한 문서들을 리스트화 한 이중 리스트\n",
        "        self.docs = docs\n",
        "        self.labels = labels\n",
        "        self.k = k #laplace smoothing을 위한 상수 k\n",
        "        self.words = words\n",
        "        self.package = package \n",
        "        self.total_token = []\n",
        "        self.unique_token = np.array((1,1))\n",
        "        self.category_name=[]\n",
        "        self.classified_token=[]\n",
        "        self.prior_p=[]\n",
        "        self.probability_dict=[]\n",
        "\n",
        "\n",
        "    def _cal_prior(self):\n",
        "        #토큰 개수 세기 \n",
        "        for doc in self.docs:\n",
        "            for token in doc:\n",
        "                self.total_token.append(token)\n",
        "        self.unique_token = np.unique(self.total_token)        \n",
        "        #카테고리별 확률 구하기\n",
        "        self.category_name = np.unique(labels)\n",
        "\n",
        "        count_labels=[]\n",
        "        for name in self.category_name:\n",
        "            tmp = []\n",
        "            for idx, doc in enumerate(self.docs):\n",
        "                if self.labels[idx] == name:\n",
        "                    tmp.extend(doc)\n",
        "            self.classified_token.append(tmp)\n",
        "            count_labels.append(len(tmp))\n",
        "        self.prior_p = list(np.array(count_labels)/len(self.total_token))\n",
        "        return dict(zip(list(self.category_name),self.prior_p))\n",
        "\n",
        "    def _cal_posterior(self):\n",
        "        self._cal_prior()\n",
        "       #각각의 카테고리에서 토큰이 나온 횟수세기        \n",
        "        ls_n_classified_token=[]            \n",
        "        for classfiedtoken in self.classified_token:\n",
        "            n_classified_token=[]\n",
        "            for token in self.unique_token:\n",
        "                tmp = classfiedtoken.count(token)\n",
        "                n_classified_token.append(tmp)\n",
        "            ls_n_classified_token.append(n_classified_token)\n",
        "\n",
        "        for i, v in enumerate(ls_n_classified_token):\n",
        "            for idx, w in enumerate(v):\n",
        "                ls_n_classified_token[i][idx] = np.log((w+1*self.k)/(sum(v)+2*self.k))\n",
        "            tmp = dict(zip(list(self.unique_token),ls_n_classified_token[i]))\n",
        "            self.probability_dict.append(tmp)\n",
        "        return self.probability_dict\n",
        "        \n",
        "    def _use_handmadeone(self):\n",
        "        self._cal_prior()\n",
        "        self._cal_posterior()\n",
        "        p_list= []\n",
        "        for idx, p in enumerate(self.prior_p): \n",
        "            tmp = np.log(p)\n",
        "            for word in self.words:\n",
        "                tmp += self.probability_dict[idx][word]\n",
        "            tmp = np.exp(tmp)\n",
        "            p_list.append(tmp) \n",
        "\n",
        "        return self.category_name[p_list.index(max(p_list))]\n",
        "\n",
        "    def _use_plain_sklearn(self):\n",
        "        text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                            ('tfidf', TfidfTransformer()),\n",
        "                            ('clf', MultinomialNB()), ])\n",
        "        og_docs = [\" \".join(i) for i in self.docs]\n",
        "        self.docs = og_docs\n",
        "        text_clf = text_clf.fit(self.docs, self.labels)\n",
        "\n",
        "        self.words = [\" \".join(self.words)]\n",
        "        return text_clf.predict(self.words)\n",
        "\n",
        "    def _use_gs_sklearn(self, parameters_dict):\n",
        "        text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                            ('tfidf', TfidfTransformer()),\n",
        "                            ('clf', MultinomialNB()), ])\n",
        "        og_docs = [\" \".join(i) for i in self.docs]\n",
        "        self.docs = og_docs\n",
        "        text_clf = text_clf.fit(self.docs, self.labels)\n",
        "        \n",
        "        # parameters_dict = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
        "        gs_clf = GridSearchCV(text_clf, parameters_dict, n_jobs=-1, verbose=2)\n",
        "        gs_clf = gs_clf.fit(self.docs, self.labels)\n",
        "\n",
        "        self.words = [\" \".join(self.words)]\n",
        "        return gs_clf.best_estimator_.get_params(), gs_clf.predict(self.words)\n",
        "\n",
        "    def classify(self):\n",
        "        if self.package == 'manual':\n",
        "            self._cal_prior()\n",
        "            self._cal_posterior()\n",
        "            return self._use_handmadeone()\n",
        "\n",
        "        elif self.package == 'sklearn':\n",
        "            return self._use_plain_sklearn()\n",
        "\n",
        "        elif self.package == 'sklearn_gs':\n",
        "            from ast import literal_eval\n",
        "            parameters_dict = input(\"조정할 파라미터 딕셔너리를 입력해주세요: \")\n",
        "            parameters_dict = literal_eval(parameters_dict) \n",
        "            return self._use_gs_sklearn(parameters_dict)\n",
        "\n",
        "        else:\n",
        "            print(\"지원하지 않는 방식입니다.\")\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4NKtjQEi3gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc = NaiveBayesClassifier_multi(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5)\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppluUCQfwh2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09d09902-f66a-41b0-b907-d958af6b815d"
      },
      "source": [
        "nbc.classify()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef-CRf-BmgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc1 = NaiveBayesClassifier_multi(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn')\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgIz83zPuq_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbc2e690-8021-4c72-d009-b3033bc16492"
      },
      "source": [
        "nbc1.classify()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-YbjYYj1Vfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc2 = NaiveBayesClassifier_multi(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn_gs')"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqWNLvjY1ZdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "9eecd74e-bb51-4544-de6a-c6b05048c727"
      },
      "source": [
        "nbc2.classify() # 데이터 부족으로 오류 생김"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "조정할 파라미터 딕셔너리를 입력해주세요: {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-692a967e8cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnbc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-1f111cc9dced>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"조정할 파라미터 딕셔너리를 입력해주세요: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_gs_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-1f111cc9dced>\u001b[0m in \u001b[0;36m_use_gs_sklearn\u001b[0;34m(self, parameters_dict)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# parameters_dict = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    661\u001b[0m             raise ValueError(\"n_splits=%d cannot be greater than the\"\n\u001b[1;32m    662\u001b[0m                              \u001b[0;34m\" number of members in each class.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                              % (self.n_splits))\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             warnings.warn((\"The least populated class in y has only %d\"\n",
            "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srG5TzkA1bsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7c557c4f-35f8-4ec2-f595-05ba76dc100e"
      },
      "source": [
        "#데이터 늘린 후 다시시도\n",
        "\n",
        "m1 = 'me free lottery'\n",
        "m2 = 'free get free you'\n",
        "m3 = 'you free scholarhip'\n",
        "m4 = 'free to contact me'\n",
        "m5 = 'you won award'\n",
        "m6 = 'you ticket lottery'\n",
        "labels = ['spam', 'spam', 'normal', 'normal', 'normal', 'spam', 'spam', 'spam', 'normal', 'normal', 'normal', 'spam']\n",
        "mails = [m1, m2, m3, m4, m5, m6, m1, m2, m3, m4, m5, m6]\n",
        "\n",
        "#토크나이징\n",
        "for i, mail in enumerate(mails):\n",
        "    mails[i] = mail.split(\" \")\n",
        "mails"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery'],\n",
              " ['me', 'free', 'lottery'],\n",
              " ['free', 'get', 'free', 'you'],\n",
              " ['you', 'free', 'scholarhip'],\n",
              " ['free', 'to', 'contact', 'me'],\n",
              " ['you', 'won', 'award'],\n",
              " ['you', 'ticket', 'lottery']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ZaL5dU1lQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "ebad331e-7894-49a1-cc15-5b333445c3ac"
      },
      "source": [
        "nbc2 = NaiveBayesClassifier_multi(docs=mails, labels=labels, words=['lottery', 'free'], k=0.5, package='sklearn_gs')\n",
        "nbc2.classify()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "조정할 파라미터 딕셔너리를 입력해주세요: {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False)}\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    0.9s remaining:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
              "  'clf__alpha': 1.0,\n",
              "  'clf__class_prior': None,\n",
              "  'clf__fit_prior': True,\n",
              "  'memory': None,\n",
              "  'steps': [('vect',\n",
              "    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                    dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                    lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                    ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                    strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                    tokenizer=None, vocabulary=None)),\n",
              "   ('tfidf',\n",
              "    TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              "   ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "  'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
              "  'tfidf__norm': 'l2',\n",
              "  'tfidf__smooth_idf': True,\n",
              "  'tfidf__sublinear_tf': False,\n",
              "  'tfidf__use_idf': True,\n",
              "  'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                  tokenizer=None, vocabulary=None),\n",
              "  'vect__analyzer': 'word',\n",
              "  'vect__binary': False,\n",
              "  'vect__decode_error': 'strict',\n",
              "  'vect__dtype': numpy.int64,\n",
              "  'vect__encoding': 'utf-8',\n",
              "  'vect__input': 'content',\n",
              "  'vect__lowercase': True,\n",
              "  'vect__max_df': 1.0,\n",
              "  'vect__max_features': None,\n",
              "  'vect__min_df': 1,\n",
              "  'vect__ngram_range': (1, 1),\n",
              "  'vect__preprocessor': None,\n",
              "  'vect__stop_words': None,\n",
              "  'vect__strip_accents': None,\n",
              "  'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "  'vect__tokenizer': None,\n",
              "  'vect__vocabulary': None,\n",
              "  'verbose': False},\n",
              " array(['spam'], dtype='<U6'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWP3xKjE1o0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}