{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA 직접 구현.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhyRwiyQkrMJavfN0zHL0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-statisticsmodel/blob/master/yykim/LDA_%EC%A7%81%EC%A0%91_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vya2Mg3rVzYr",
        "colab_type": "text"
      },
      "source": [
        "## LDA 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjlt7g5wWK6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = 'Cute kitty'\n",
        "d2 = 'Eat rice or cake'\n",
        "d3 = 'Kitty and hamster'\n",
        "d4 = 'Eat bread'\n",
        "d5 = 'Rice, bread and cake'\n",
        "d6 = 'Cute hamster eats bread and cake'\n",
        "\n",
        "docs = [d1, d2, d3, d4, d5,d6]\n",
        "docs = [i.replace(\",\",\"\") for i in docs]\n",
        "docs = [i.replace(\"!\",\"\") for i in docs]\n",
        "docs = [i.replace(\".\",\"\") for i in docs]\n",
        "docs = [i.replace(\"\\'\",\"\") for i in docs]\n",
        "docs = [i.replace(\"\\\"\",\"\") for i in docs]\n",
        "docs = [i.replace(\":\",\"\") for i in docs]\n",
        "docs = [i.replace(\";\",\"\") for i in docs]\n",
        "docs = [i.replace(\"and\",\"\") for i in docs]\n",
        "docs = [i.replace(\"or\",\"\") for i in docs]\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HhEXi11X8Tp",
        "colab_type": "text"
      },
      "source": [
        "##Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Drt8N_X9Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001, max_iter = 50):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = topic_num\n",
        "        self.max_iter = max_iter\n",
        "        self.total_tokens = []\n",
        "        self.unique_total_tokens = []\n",
        "        self.doc_ls = doc_ls\n",
        "    \n",
        "    def _assign_topic_randomly(self):\n",
        "        tokenized_docs = [doc.split(\" \") for doc in self.doc_ls] #문서별로 토큰화\n",
        "\n",
        "        self.total_tokens = [] #문서 구분 없이 토큰화, 유니크 토큰 뽑기\n",
        "        for sentence in self.doc_ls:\n",
        "            self.total_tokens.extend(sentence.split(\" \"))\n",
        "        self.unique_total_tokens = list(np.unique(self.total_tokens))\n",
        "\n",
        "        docs_tokens_topic = tokenized_docs.copy()\n",
        "        for i, tokenized_doc in enumerate(docs_tokens_topic): #문서별로 토큰화된 토큰에 랜덤으로 토픽 부여\n",
        "            docs_tokens_topic[i] = [ random.randint(0,100) % self.k for token in tokenized_doc]\n",
        "\n",
        "        return docs_tokens_topic\n",
        "\n",
        "    def _cal_topic_in_doc(self):        \n",
        "        docs_tokens_topic = self._assign_topic_randomly()\n",
        "        topic_in_doc = []\n",
        "        for topic in range(self.k):\n",
        "            tmp = []\n",
        "            for doc_tokens_topic in docs_tokens_topic:\n",
        "                    tmp.append(doc_tokens_topic.count(topic))\n",
        "            topic_in_doc.append(tmp)\n",
        "        return np.array(topic_in_doc) + self.alpha\n",
        "\n",
        "    def _cal_words_in_topic(self):\n",
        "        docs_tokens_topic = self._assign_topic_randomly()\n",
        "        topic_token = []\n",
        "        for doc in docs_tokens_topic:\n",
        "            topic_token.extend(doc)\n",
        "\n",
        "        topics_count_per_token=[]\n",
        "        for u_token in self.unique_total_tokens:\n",
        "            tmp = []\n",
        "            topic_count = []\n",
        "            for idx, token in enumerate(self.total_tokens):\n",
        "                if token == u_token:\n",
        "                    tmp.append(topic_token[idx])\n",
        "            for topic in range(self.k):\n",
        "                topic_count.append(tmp.count(topic))\n",
        "            topics_count_per_token.append(topic_count)\n",
        "        return np.array(topics_count_per_token).T + self.beta\n",
        "\n",
        "    def fit(self):\n",
        "        docs_tokens_topic = self._assign_topic_randomly()\n",
        "        pre_docs_tokens_topic = docs_tokens_topic.copy()\n",
        "\n",
        "        for iter in range(self.max_iter):\n",
        "            for i, doc in enumerate([doc.split(\" \") for doc in self.doc_ls]):\n",
        "                for ix, token in enumerate(doc):\n",
        "                    idx = self.unique_total_tokens.index(token) #토큰의 인덱스\n",
        "                    which_topic = [] \n",
        "                    for topic in range(self.k):\n",
        "                        if topic == docs_tokens_topic[i][ix]:\n",
        "                            which_topic.append(((self._cal_topic_in_doc()[topic][i]-1)/(self._cal_topic_in_doc()[:,[i]].sum()-1))*((self._cal_words_in_topic()[topic][idx]-1)/(self._cal_words_in_topic()[topic].sum()-1)))\n",
        "                        else:\n",
        "                            which_topic.append((self._cal_topic_in_doc()[topic][i]/(self._cal_topic_in_doc()[:,[i]].sum()-1))*(self._cal_words_in_topic()[topic][idx]/(self._cal_words_in_topic()[topic].sum()-1)))\n",
        "                    pre_docs_tokens_topic[i][ix] = np.argmax(np.array(which_topic))\n",
        "\n",
        "            if docs_tokens_topic == pre_docs_tokens_topic:\n",
        "                break    \n",
        "\n",
        "            docs_tokens_topic = pre_docs_tokens_topic\n",
        "\n",
        "    def predict(self):\n",
        "        self._assign_topic_randomly()\n",
        "        self._cal_topic_in_doc()\n",
        "        words_in_tp = self._cal_words_in_topic()\n",
        "        self.fit()\n",
        "        for topic in range(self.k):\n",
        "            np.argsort(words_in_tp[topic])[::-1][:4]\n",
        "            top = np.argsort(words_in_tp)[topic][::-1][:4]\n",
        "            tmp=[]\n",
        "            for i in top:\n",
        "                tmp.append((self.unique_total_tokens[i], (words_in_tp[topic][i]/(words_in_tp[topic].sum()-1))))\n",
        "            print(\"Topic{}: {}\".format(topic, tmp))\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN8SC_RJh2fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LDA(doc_ls = docs, topic_num=2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nw6juYYiAq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "793de03c-b0ca-4402-ae46-a066edf632ed"
      },
      "source": [
        "lda.predict()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic0: [('cake', 0.2725456361819999), ('', 0.2725456361819999), ('bread', 0.1817273635455454), ('hamster', 0.09090909090909093)]\n",
            "Topic1: [('rice', 0.12495318936462367), ('kitty', 0.12495318936462367), ('hamster', 0.12495318936462367), ('bread', 0.12495318936462367)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUWLVlKViCFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}